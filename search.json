[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SampleBlog",
    "section": "",
    "text": "This is a blog post on probability theory and random variables.\n\n\n\n\n\n\n\nProbability Theory, Random Variables, Naive Bayes\n\n\n\n\nThis is a blog post on probability theory. Specifically, the Naive Bayes classifier is used.\n\n\n\n\n\n\nNov 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nThis is a blog post on anomaly detection.\n\n\n\n\n\n\n\nAnomaly Detection\n\n\n\n\nThis is a blog post on anomaly detection. Specifically, we will analyze the DBSCAN algorithm.\n\n\n\n\n\n\nOct 31, 2023\n\n\n\n\n\n\n  \n\n\n\n\nThis is a blog post on clustering\n\n\n\n\n\n\n\nClustering\n\n\n\n\nThis is a blog post on clustering.\n\n\n\n\n\n\nOct 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nThis is a blog post on classification\n\n\n\n\n\n\n\nClassification\n\n\n\n\nThis is a blog post on classification. Specifically, the SVM classifier is used.\n\n\n\n\n\n\nOct 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\nThis is a blog post on regression.\n\n\n\n\n\n\n\nLinear Regression\n\n\nNonlinear Regression\n\n\n\n\nThis is a blog post on regression. I will use the Seaborn Diamonds dataset to predict the price of a diamond based on other features about it.\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\nNo matching items\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/"
  },
  {
    "objectID": "posts/prob_theory/index.html",
    "href": "posts/prob_theory/index.html",
    "title": "This is a blog post on probability theory and random variables.",
    "section": "",
    "text": "In this blog post, we will investigate the Seaborn Attention dataset. Specifically, we will focus on using the Naive Bayes classifier to predict whether a test-taker is focused or not depending on their score.\n\nGet and Examine the Data\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsns.get_dataset_names()\nattention_raw = sns.load_dataset('attention')\nattention_raw.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nsubject\nattention\nsolutions\nscore\n\n\n\n\n0\n0\n1\ndivided\n1\n2.0\n\n\n1\n1\n2\ndivided\n1\n3.0\n\n\n2\n2\n3\ndivided\n1\n3.0\n\n\n3\n3\n4\ndivided\n1\n5.0\n\n\n4\n4\n5\ndivided\n1\n4.0\n\n\n\n\n\n\n\n\nattention_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 60 entries, 0 to 59\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Unnamed: 0  60 non-null     int64  \n 1   subject     60 non-null     int64  \n 2   attention   60 non-null     object \n 3   solutions   60 non-null     int64  \n 4   score       60 non-null     float64\ndtypes: float64(1), int64(3), object(1)\nmemory usage: 2.5+ KB\n\n\nLet’s drop the Unnamed column, since it will not help with our analysis.\n\nattention = attention_raw.drop(\"Unnamed: 0\", axis=1)\nattention.head()\n\n\n\n\n\n\n\n\nsubject\nattention\nsolutions\nscore\n\n\n\n\n0\n1\ndivided\n1\n2.0\n\n\n1\n2\ndivided\n1\n3.0\n\n\n2\n3\ndivided\n1\n3.0\n\n\n3\n4\ndivided\n1\n5.0\n\n\n4\n5\ndivided\n1\n4.0\n\n\n\n\n\n\n\nLet’s explore the dataset and see if there are any missing/outlier values.\n\nattention.isna().any().any()\n\nFalse\n\n\n\nattention.shape\n\n(60, 4)\n\n\n\n[attention['subject'].min(), attention['subject'].max()]\n\n[1, 20]\n\n\n\n[attention['solutions'].min(), attention['solutions'].max()]\n\n[1, 3]\n\n\n\n[attention['score'].min(), attention['score'].max()]\n\n[2.0, 9.0]\n\n\nIt looks like there are 20 subjects total, who were given 3 different tests. From each of these tests, the score of the subjects was computed. The minimum score across all tests was 2.0, while the maximum score was 9.0. Presumably, the test was ranked on a scale of 1 to 10.\n\n\nVisualizations\nLet’s now visualize the attention dataset, by plotting the distribution of participant scores.\n\nsns.barplot(data=attention, x='subject', y='score');\n\n\n\n\nIt appears that some subjects performed better than others. Let’s visualize which of the subjects were focused and which were distracted.\n\nsns.barplot(data=attention, x='subject', y='score', hue='attention');\n\n\n\n\nIt appears that, on average, focused subjects appeared to perform better than non-focused subjects. Let’s confirm this hypothesis by computing the average scores for the divided and focused groups.\n\navg_scores = pd.DataFrame(attention.groupby('attention').mean()['score'])\navg_scores\n\n\n\n\n\n\n\n\nscore\n\n\nattention\n\n\n\n\n\ndivided\n5.116667\n\n\nfocused\n6.800000\n\n\n\n\n\n\n\nIndeed, we can see that the average score for divided users is around 5, while the average score for focused users is approximately 6.8. It appears that the average test score for focused users is almost 2 points higher than that of distracted users.\n\n\nNaive Bayes Classifier\nLet’s now use a Naive Bayes classifier to predict whether a participant is distracted or not based on their score. We can drop the other columns, since they won’t be relevant.\n\nattention_nb = attention[['attention', 'score']]\nattention_nb.head()\n\n\n\n\n\n\n\n\nattention\nscore\n\n\n\n\n0\ndivided\n2.0\n\n\n1\ndivided\n3.0\n\n\n2\ndivided\n3.0\n\n\n3\ndivided\n5.0\n\n\n4\ndivided\n4.0\n\n\n\n\n\n\n\nNext, perform a train-test split. Since the dataset is small, let’s choose 80% of the samples to be in the training set and 20% of the samples to be in the test set.\n\nfrom sklearn.model_selection import train_test_split\n\nX = attention_nb[['score']]\ny = attention_nb['attention']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n\nX_train.shape\nX_test.shape\ny_train.shape\ny_test.shape\n\n(12,)\n\n\nNow, let’s instantiate the model and fit the data to the model. We will use a Gaussian Naive Bayes classifier, which assumes that the distribution of continuous features is Gaussian. This appears to be a reasonable assumption in the case of the test scores.\n\nfrom sklearn.naive_bayes import GaussianNB\n\nnb_gauss = GaussianNB()\nnb_gauss.fit(X_train, y_train);\n\nLet’s evaluate the accuracy on our testing set.\n\npredictions = nb_gauss.predict(X_test)\n\n\nfrom sklearn.metrics import accuracy_score as accuracy\nround(accuracy(y_test, predictions), 2)\n\n0.75\n\n\n\npredictions\n\narray(['divided', 'divided', 'focused', 'focused', 'focused', 'focused',\n       'focused', 'focused', 'focused', 'focused', 'focused', 'focused'],\n      dtype='&lt;U7')\n\n\n75% is actually quite good, considering how few samples were used to train the model. We can plot a confusion matrix to get a better sense for which samples the model misclassified.\n\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nlabels_arr = ['divided', 'focused']\nconf_matrix = metrics.confusion_matrix(y_test, predictions, labels=labels_arr)\ncm = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['divided', 'focused'])\ncm.plot();\n\n\n\n\nIt appears that the Gaussian NB classifier over-predicted samples as being focused. The three samples in the upper right quadrant were predicted as being focused, but were in fact divided.\n\n\nAttributions\n\nhttps://seaborn.pydata.org/generated/seaborn.barplot.html\nhttps://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\nhttps://scikit-learn.org/stable/modules/naive_bayes.html\nhttps://www.datacamp.com/tutorial/naive-bayes-scikit-learn"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "This is a blog post on clustering",
    "section": "",
    "text": "In this blog post, I will use the Seaborn “Car Crashes” dataset in order to group similar states together and observe patterns with motor accidents in these states."
  },
  {
    "objectID": "posts/clustering/index.html#elbow-curve",
    "href": "posts/clustering/index.html#elbow-curve",
    "title": "This is a blog post on clustering",
    "section": "Elbow Curve",
    "text": "Elbow Curve\nLet’s draw an elbow curve to visualize how the inertia drops as the number of clusters increases.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nkmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(crashes_without_abbrev)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.xlabel(\"$k$\");\nplt.ylabel(\"Inertia\");\nplt.grid()\nplt.show()\n# plt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n#              arrowprops=dict(facecolor='black', shrink=0.1))\n\n\n\n\nFrom this plot, it appears that in fact k=2 is the inflection point of the graph. Let’s also construct a silhouette plot.\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_scores = [silhouette_score(crashes_without_abbrev, model.labels_)\n                     for model in kmeans_per_k[1:]]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(2, 10), silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Silhouette score\")\n\nplt.grid()\nplt.show()\n\n\n\n\nIt appears that 2 clusters is the best choice according to both the elbow curve and the silhouette score. There doesn’t appear to be a good reason to not choose 2 clusters, so let’s proceed with the KMeans algorithm using 2 clusters.\n\nkmeans = KMeans(n_clusters=2, random_state=42)\ny_pred = kmeans.fit_predict(crashes_without_abbrev)\ny_pred\n\narray([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0], dtype=int32)\n\n\n\nax = crashes2D.plot.scatter(x=0, y=1, figsize=(10,7), c=y_pred, cmap='viridis', s=50)\nfor ind,row in crashes2D.iterrows():\n    ax.text(row[0], row[1], ind)\nax.set_xlabel('PCA Component 1 (98.67%)');\nax.set_ylabel('PCA Component 2 (1.16%)');\nax.axvline(x=85, color='r')\n\n&lt;matplotlib.lines.Line2D at 0x7fa0789b4640&gt;\n\n\n\n\n\nFrom this plot, we can see a clear split along PC1. The vertical line on the plot separates the two clusters perfectly. We can see that states to the right of that line belong to one cluster and states to the left of the vertical line lie in the other cluster. Since PC1 explains over 98% of the variance, this approach would make sense."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "This is a blog post on classification",
    "section": "",
    "text": "# Necessary data science packages\nimport sys\nfrom packaging import version\nimport sklearn\nimport sklearn.datasets\nimport pandas as pd\nimport numpy as np\n\nwine_dataset = sklearn.datasets.load_wine()\nwine_df = pd.DataFrame(data=wine_dataset.data, columns=wine_dataset.feature_names)\nwine_df['labels'] = wine_dataset['target'] # Also add the labels associated with each sample\n\nX, y = wine_dataset.data, wine_dataset.target\n\n\nX.shape\n\n(178, 13)\n\n\n\ny\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])\n\n\n\ny.shape\n\n(178,)\n\n\n\n\n\n\n# Display the head of the wine dataframe. \nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nlabels\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0\n\n\n\n\n\n\n\n\n# Display column names and types\nwine_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       178 non-null    float64\n 1   malic_acid                    178 non-null    float64\n 2   ash                           178 non-null    float64\n 3   alcalinity_of_ash             178 non-null    float64\n 4   magnesium                     178 non-null    float64\n 5   total_phenols                 178 non-null    float64\n 6   flavanoids                    178 non-null    float64\n 7   nonflavanoid_phenols          178 non-null    float64\n 8   proanthocyanins               178 non-null    float64\n 9   color_intensity               178 non-null    float64\n 10  hue                           178 non-null    float64\n 11  od280/od315_of_diluted_wines  178 non-null    float64\n 12  proline                       178 non-null    float64\n 13  labels                        178 non-null    int64  \ndtypes: float64(13), int64(1)\nmemory usage: 19.6 KB\n\n\n\n# Get the value counts for each different type of wine. \nwine_df['labels'].value_counts()\n\nlabels\n1    71\n0    59\n2    48\nName: count, dtype: int64\n\n\nIt seems that the labels are roughly balanced, although wine type #1 is the most common. Now, let’s do a train-test split.\n\ndef shuffle_and_split_data(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n\ntrain_set, test_set = shuffle_and_split_data(wine_df, 0.2)\nlen(train_set)\n\n143\n\n\n\n# Allows results of the notebook to be reproducible. \nnp.random.seed(42)\n\n\nlen(test_set)\n\n35"
  },
  {
    "objectID": "posts/classification/index.html#download-data",
    "href": "posts/classification/index.html#download-data",
    "title": "This is a blog post on classification",
    "section": "",
    "text": "# Necessary data science packages\nimport sys\nfrom packaging import version\nimport sklearn\nimport sklearn.datasets\nimport pandas as pd\nimport numpy as np\n\nwine_dataset = sklearn.datasets.load_wine()\nwine_df = pd.DataFrame(data=wine_dataset.data, columns=wine_dataset.feature_names)\nwine_df['labels'] = wine_dataset['target'] # Also add the labels associated with each sample\n\nX, y = wine_dataset.data, wine_dataset.target\n\n\nX.shape\n\n(178, 13)\n\n\n\ny\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])\n\n\n\ny.shape\n\n(178,)"
  },
  {
    "objectID": "posts/classification/index.html#inspect-the-data",
    "href": "posts/classification/index.html#inspect-the-data",
    "title": "This is a blog post on classification",
    "section": "",
    "text": "# Display the head of the wine dataframe. \nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nlabels\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0\n\n\n\n\n\n\n\n\n# Display column names and types\nwine_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       178 non-null    float64\n 1   malic_acid                    178 non-null    float64\n 2   ash                           178 non-null    float64\n 3   alcalinity_of_ash             178 non-null    float64\n 4   magnesium                     178 non-null    float64\n 5   total_phenols                 178 non-null    float64\n 6   flavanoids                    178 non-null    float64\n 7   nonflavanoid_phenols          178 non-null    float64\n 8   proanthocyanins               178 non-null    float64\n 9   color_intensity               178 non-null    float64\n 10  hue                           178 non-null    float64\n 11  od280/od315_of_diluted_wines  178 non-null    float64\n 12  proline                       178 non-null    float64\n 13  labels                        178 non-null    int64  \ndtypes: float64(13), int64(1)\nmemory usage: 19.6 KB\n\n\n\n# Get the value counts for each different type of wine. \nwine_df['labels'].value_counts()\n\nlabels\n1    71\n0    59\n2    48\nName: count, dtype: int64\n\n\nIt seems that the labels are roughly balanced, although wine type #1 is the most common. Now, let’s do a train-test split.\n\ndef shuffle_and_split_data(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n\ntrain_set, test_set = shuffle_and_split_data(wine_df, 0.2)\nlen(train_set)\n\n143\n\n\n\n# Allows results of the notebook to be reproducible. \nnp.random.seed(42)\n\n\nlen(test_set)\n\n35"
  },
  {
    "objectID": "posts/classification/index.html#train-test-split",
    "href": "posts/classification/index.html#train-test-split",
    "title": "This is a blog post on classification",
    "section": "Train Test Split",
    "text": "Train Test Split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\nX_train.shape\nX_test.shape\ny_train.shape\ny_test.shape\n\n(59,)\n\n\nWe have successfully performed a train-test split with 143 samples in the training set and 35 samples in the test set. Since we are dealing with a small dataset for multi-class classification, it might be helpful to use the SVM classifier.\n\nfrom sklearn.svm import SVC\n\n# Instantiate and fit on the training set. \nsvm_clf = SVC(random_state=42)\nsvm_clf.fit(X_train, y_train)\n\nSVC(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(random_state=42)\n\n\n\n# Predict on the testing set. \npredictions = svm_clf.predict(X_test)\n\n\n# Now, let's evaluate the accuracy of the predictions. \nfrom sklearn.metrics import accuracy_score as accuracy\nround(accuracy(y_test, predictions), 2)\n\n0.71\n\n\nWe get an accuracy of around 71%. Let’s plot our results in a confusion matrix.\n\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nconf_matrix = metrics.confusion_matrix(y_test, predictions)\ncm = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[0,1,2])\ncm.plot();\n\n\n\n\nThere are quite a few misclassified samples. Let’s try using a different model to see if we get better results. We can use a Decision Tree. To avoid overfitting, let’s set the max_depth to be 2.\n\nfrom sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf.fit(X, y)\n\nDecisionTreeClassifier(max_depth=2, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=2, random_state=42)\n\n\nNow, let’s visualize the resulting dendrogram.\n\nfrom sklearn.tree import export_graphviz\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n#Define the class labels. \nclass_labels_str = [str(elem) for elem in set(y)]\nwine_classes = np.array(class_labels_str)\n\nplt.figure(figsize=(12,12))\n\n\ndendro = sklearn.tree.plot_tree(tree_clf, filled=True, rounded=True, feature_names=wine_dataset.feature_names, \n                                class_names=wine_classes, fontsize=12)\n\nplt.show()\n\n\n\n\nFrom the dendrogram, we can see that there are less misclassifications than the support vector machine model. However, while the SVC model had 0 misclassifications for class 0, the DecisionTree misclassifies two samples as belonging to class 0."
  },
  {
    "objectID": "posts/anomaly_detection/index.html",
    "href": "posts/anomaly_detection/index.html",
    "title": "This is a blog post on anomaly detection.",
    "section": "",
    "text": "In this blog post, I will continue using the Seaborn “Car Crashes” dataset. Previously, we analyzed the states in this dataset by using the k-means clustering algorithm. In this blog post, we will continue with this analysis, but instead use the DBSCAN algorithm. DBSCAN will allow us to identify states which are outliers/anomalies."
  },
  {
    "objectID": "posts/anomaly_detection/index.html#dbscan-algorithm",
    "href": "posts/anomaly_detection/index.html#dbscan-algorithm",
    "title": "This is a blog post on anomaly detection.",
    "section": "DBSCAN Algorithm",
    "text": "DBSCAN Algorithm\nNow, we are ready to use the DBScan algorithm to identify the outlier states. This algorithm takes in two parameters: eps and min_samples. This algorithm finds core points and then expands outwards from those core points to form clusters.\n\nfrom sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=30, min_samples=2)\ndbscan.fit(crashes_without_abbrev)\n\nDBSCAN(eps=30, min_samples=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCANDBSCAN(eps=30, min_samples=2)\n\n\n\ndbscan.labels_\n\narray([ 0,  1,  2,  0,  3,  0, -1,  4, -1,  4,  3,  2,  5,  0,  5,  5,  0,\n        2, -1,  5, -1,  1,  4,  0,  3,  0,  0,  5,  1,  5, -1,  2, -1,  5,\n        5,  5,  3,  0,  3,  4,  2,  5,  0,  1,  0,  5,  0,  2,  1,  5,  0])\n\n\nWe had to make our eps quite large and the min. samples small in order to get meaningful clusters. Otherwise, there were just too many outliers.\nLet’s now plot these cluster assignments on our original PCA plot.\n\nax = crashes2D.plot.scatter(x=0, y=1, figsize=(10,7), c=dbscan.labels_, cmap='tab10', s=50)\nfor ind,row in crashes2D.iterrows():\n    ax.text(row[0], row[1], ind)\nax.set_xlabel('PCA Component 1 (98.67%)');\nax.set_ylabel('PCA Component 2 (1.16%)');\n# ax.axvline(x=85, color='r')\n\n\n\n\nLet’s try to get a list of the outliers identified by the DBSCAN algorithm.\n\noutliers = [crashes2D.index[i] for i in range(len(dbscan.labels_)) if dbscan.labels_[i] == -1]\noutliers\n\n['CT', 'DC', 'LA', 'MD', 'NJ', 'NY']\n\n\nThe DBSCAN algorithm, with eps of 30 and min_samples of 2, identifies 6 states/districts above as outliers (‘CT’, ‘DC’, ‘LA’, etc.).\nIt seems that from the plot, some states, like LA and MD, are so far from the rest of the states that they are clear outliers. Additionally, even though ‘CT’ might appear close to the red cluster, remember that the PCA plot is a 2-D representation of high-dimensional data. Therefore, ‘CT’ is actually probably much further away from the red cluster in reality."
  },
  {
    "objectID": "posts/anomaly_detection/index.html#gaussian-mixtures-algorithm",
    "href": "posts/anomaly_detection/index.html#gaussian-mixtures-algorithm",
    "title": "This is a blog post on anomaly detection.",
    "section": "Gaussian Mixtures Algorithm",
    "text": "Gaussian Mixtures Algorithm\nThe Gaussian Mixture algorithm assumes that the data is a combination of several different normal (Gaussian) distributions. A given data point will be added to a cluster if it has the highest probability of belonging to that cluster as opposed to other clusters.\nLet’s try running the Gaussian Mixture Algorithm in order to determine the anomalies in the crashes dataset. We can tune the value of num_components (i.e. number of clusters). Also, let’s set n_inits to be 10.\n\nfrom sklearn.mixture import GaussianMixture\ngm = GaussianMixture(n_components=2, n_init=10, random_state=42)\ngm.fit(crashes_without_abbrev)\n\nGaussianMixture(n_components=2, n_init=10, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=2, n_init=10, random_state=42)\n\n\nLet’s take a look at the parameters that were obtained from the training process.\n\ngm.weights_\n\narray([0.57719335, 0.42280665])\n\n\n\ngm.means_\n\narray([[  16.37076754,    4.90459755,    5.09393917,   13.59724703,\n          14.457428  ,  774.68844487,  120.18638301],\n       [  14.99763053,    5.12597186,    4.60398743,   13.54031661,\n          13.3870909 , 1040.22164212,  154.02396378]])\n\n\n\ngm.converged_\n\nTrue\n\n\n\ngm.n_iter_\n\n21\n\n\nIt looks like the algorithm converged after 21 iterations. Now, let’s take a look at the predicted class labels as well as the probability distribution between the two classes.\n\ngm_labels = gm.predict(crashes_without_abbrev)\ngm_labels\n\narray([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n       1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0])\n\n\n\ngm.predict_proba(crashes_without_abbrev).round(3)[:5]\n\narray([[0.091, 0.909],\n       [0.009, 0.991],\n       [1.   , 0.   ],\n       [0.947, 0.053],\n       [0.013, 0.987]])\n\n\nAs we can see, there are two classes, with labels 0 and 1. The probability that each of the first 5 samples belongs to class 0 and class 1, respectively, are shown in the cell above.\nLet’s now try to visualize the cluster assignments given by the Gaussian Mixture on the PCA plot.\n\nax = crashes2D.plot.scatter(x=0, y=1, figsize=(10,7), c=gm_labels, cmap='viridis', s=50)\nfor ind,row in crashes2D.iterrows():\n    ax.text(row[0], row[1], ind)\nax.set_xlabel('PCA Component 1 (98.67%)');\nax.set_ylabel('PCA Component 2 (1.16%)');\n\n\n\n\nUnsurprisingly, the samples are again split primarily based off of Principal Component 1."
  },
  {
    "objectID": "posts/anomaly_detection/index.html#gaussian-mixtures-for-anomaly-detection",
    "href": "posts/anomaly_detection/index.html#gaussian-mixtures-for-anomaly-detection",
    "title": "This is a blog post on anomaly detection.",
    "section": "Gaussian Mixtures for Anomaly Detection",
    "text": "Gaussian Mixtures for Anomaly Detection\nWe can use the trained Gaussian Mixture model in order to predict which observations are outliers. Visually, it appears that there should be at least 2 or 3 outliers. As a first attempt, we can set a density threshold at the 5th percentile. This would account for approximately 2-3 samples (0.05 * 51 = 2.55). All of the samples that fall below this threshold will be classified as outliers.\n\nimport numpy as np\n\ndensities = gm.score_samples(crashes_without_abbrev)\ndensity_threshold = np.percentile(densities, 5)\nanomalies_below = crashes_without_abbrev[densities &lt; density_threshold]\nanomalies_below\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\n\n\nabbrev\n\n\n\n\n\n\n\n\n\n\n\nKY\n21.4\n4.066\n4.922\n16.692\n16.264\n872.51\n137.13\n\n\nMS\n17.6\n2.640\n5.456\n1.760\n17.600\n896.07\n155.77\n\n\nND\n23.9\n5.497\n10.038\n23.661\n20.554\n688.75\n109.72\n\n\n\n\n\n\n\nWhen using Gaussian Mixtures, Kentucky, Mississippi, and North Dakota are classified as outliers. This is an interesting result, since DBSCAN identified a different set of outliers.\nOne possible explanation, is that DBSCAN has difficulty with sparse points, while Gaussian Mixtures is more robust to sparse points. Gaussian Mixtures instead focuses on the probability that a sample belongs to a given cluster or not. Hence, the DBSCAN and Gaussian Mixtures algorithms may identify a different set of outliers in some cases."
  },
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "This is a blog post on regression.",
    "section": "",
    "text": "In this blog post, I will use the Seaborn Diamonds dataset to predict the price of a diamond based on various features of that diamond, like color, cut, and depth. Through this exercise, I hope to learn which features of a diamond contribute most to its price.\n\nGet and Examine the Data\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# sns.get_dataset_names()\ndiamonds = sns.load_dataset('diamonds')\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\nLet’s examine the columns and their data types. We will also check for null and missing values.\n\ndiamonds.shape\ndiamonds.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 53940 entries, 0 to 53939\nData columns (total 10 columns):\n #   Column   Non-Null Count  Dtype   \n---  ------   --------------  -----   \n 0   carat    53940 non-null  float64 \n 1   cut      53940 non-null  category\n 2   color    53940 non-null  category\n 3   clarity  53940 non-null  category\n 4   depth    53940 non-null  float64 \n 5   table    53940 non-null  float64 \n 6   price    53940 non-null  int64   \n 7   x        53940 non-null  float64 \n 8   y        53940 non-null  float64 \n 9   z        53940 non-null  float64 \ndtypes: category(3), float64(6), int64(1)\nmemory usage: 3.0 MB\n\n\n\n# Check for any null values. \ndiamonds.isna().any().any()\n\nFalse\n\n\nLooks like this dataset doesn’t contain any null values, so we don’t have to interpolate missing values.\n\n# Let's find the minimum and maximum price for any individual diamond. \n[diamonds['price'].min(), diamonds['price'].max()]\n\n[326, 18823]\n\n\nIt appears the price of the diamonds is in a very large range. The price varies from a couple hundred dollars on the low-end to around $19K on the high end. Let’s also examine the other quantitative variables, to ensure they are correct.\n\n[diamonds['carat'].min(), diamonds['carat'].max()]\n[diamonds['depth'].min(), diamonds['depth'].max()]\n[diamonds['table'].min(), diamonds['table'].max()]\n[diamonds['x'].min(), diamonds['x'].max()]\n\n[0.0, 10.74]\n\n\nSo far, carat, depth, and table seem to have a reasonable range. However, the x-min is 0.0. It is not possible for the x-dimension of the diamond to be zero (otherwise it would be flat), so this seems suspicious. Let’s see if the same problem appears for the y and z dimensions.\n\n[diamonds['y'].min(), diamonds['z'].min()]\n\n[0.0, 0.0]\n\n\nIndeed, the y and z dimensions also have min values of 0. Let’s remove these entries from the dataframe before we do any further processing.\n\ndiamonds = diamonds[(diamonds[['x', 'y', 'z']] != 0).all(axis=1)]\ndiamonds.shape\n\n(53920, 10)\n\n\n\n# Print out the new minimum values for all three variables. \n[diamonds['x'].min(), diamonds['y'].min(), diamonds['z'].min()]\n\n[3.73, 3.68, 1.07]\n\n\nThese new minimum values look much better.\nLet’s now get a better sense for the price variability by plotting the diamond price via a histogram.\n\nsns.histplot(data=diamonds, x=\"price\")\n\n&lt;Axes: xlabel='price', ylabel='Count'&gt;\n\n\n\n\n\nThis visualization gives us further insight into the price of the diamonds. It seems that most of the diamonds are on the low end, but there are a few very expensive diamonds.\nNow, let’s identify which factors most affect a given diamond’s price. First, let’s identify what factor the diamond carat (weight) has on price. We can visualize this relationship through a scatterplot.\n\nsns.scatterplot(data=diamonds, x=\"carat\", y=\"price\", size=1, alpha=0.6, edgecolor=None)\n\n&lt;Axes: xlabel='carat', ylabel='price'&gt;\n\n\n\n\n\nIt seems there is a lot of noise that somewhat degrades the quality of the plot. There is a lot of variation in price, especially for smaller carat values. However, there appears to be a positive relationship between carat value and price. We may, however, need to combine the diamond’s weight with other features of the diamond in order to help determine the price.\nSince the linear regression model will be easier to construct with quantitative variables, we will not include the following features of the diamond into our linear model. However, it may be worthwhile to see what impact they have on price. We can first examine how the diamond cut affects price through a bar chart.\n\nsns.barplot(data=diamonds, x=\"cut\", y=\"price\")\n\n&lt;Axes: xlabel='cut', ylabel='price'&gt;\n\n\n\n\n\nInterestingly, it appears that the quality of the cut doesn’t have much to do with the price. In fact, ideal cuts appear to have the lowest aggregated price.\nLet’s also examine if the color of the diamond is a good predictor of price.\n\nsns.barplot(data=diamonds, x=\"color\", y=\"price\")\n\n&lt;Axes: xlabel='color', ylabel='price'&gt;\n\n\n\n\n\nHere, we see that as the color changes from D (worst) to J (best), the price tends to increase.\n\n\nConstructing a Linear Model\nNow, we are ready to create a linear model, which will allow us to predict the price of a diamond given its carat, color, clarity, depth, etc.\nFirst, let’s try to predict the price based on just its carat and depth. We can add more variables later, but let’s keep it simple for now. We will begin by performing a train-test split.\n\nfrom sklearn.model_selection import train_test_split\nX = diamonds[[\"carat\", \"depth\"]]\ny = diamonds['price']\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.10, random_state=42)\n\n\nfrom sklearn.linear_model import LinearRegression\nlin_reg1 = LinearRegression().fit(X_train1, y_train1)\nlin_reg1.coef_,lin_reg1.intercept_\n\n(array([7762.28116317, -103.52648711]), 4132.415850047029)\n\n\nIt appears that a diamond’s carat value has a strong positive correlation with the diamond’s price, while the diamond depth has a negative correlation with price. Let’s see how well this linear model predicts the price of samples in the test set.\n\ny_pred1 = lin_reg1.predict(X_test1)\n\n# Compute the R^2 score for the test set. \nlin_reg1.score(X_test1, y_test1)\n\n0.8616559045890192\n\n\nOur R^2 score is around 86%, which is pretty good. This metric indicates that most of the variability in the data can be explained by the model.\nLet’s also construct a linear model using the diamond’s dimensions (x, y, z) as the input variables and price as the output variable.\n\nX = diamonds[['x', 'y', 'z']]\ny = diamonds['price']\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.10, random_state=42)\n\nlin_reg2 = LinearRegression().fit(X_train2, y_train2)\nlin_reg2.coef_, lin_reg2.intercept_\n\n(array([2789.14417141,  211.45956175,  265.86458654]), -14207.749973068772)\n\n\nAs the dimensions of the diamond increase, its price also tends to increase. This makes intuitive sense, since larger diamonds are probably worth more than smaller diamonds.\nBelow are the predictions for this second linear model.\n\ny_pred2 = lin_reg2.predict(X_test2)\nlin_reg2.score(X_test2, y_test2)\n\n0.7931234273030509\n\n\nAgain, we get an R^2 score of around 80%. It could be better, but it’s certainly not bad.\n\n\nVisualizations of Linear Model\nNow, I will present a few visualizations of the diamonds dataset. I will start by plotting the carat and depth values and observing how they affect price.\n\nax = sns.scatterplot(data=diamonds, x=\"carat\", y=\"depth\", hue=\"price\", size=\"price\")\n\n\n\n\nWe can immediately see that as the diamond carat increases, the price tends to increase. This can be established due to the darker dots beginning from ~1 carat and extending until ~5 carats. The relationship between depth and price is less apparent.\nNow, let’s visualize the relationship between the dimensions of the diamond (x, y, z) and price. We can begin by visualizing the price values between each pair of the diamond’s dimensions (e.g. x vs. y, x vs. z, and y vs. z).\n\nax = sns.scatterplot(data=diamonds, x=\"x\", y=\"y\", hue=\"price\", size=\"price\")\nax.set_ylim(ymin=3,ymax=11);\nax.set_xlim(xmin=3, xmax=11);\n\n\n\n\n\nax = sns.scatterplot(data=diamonds, x=\"x\", y=\"z\", hue=\"price\", size=\"price\")\nax.set_ylim(ymin=1, ymax=9);\n\n\n\n\n\nax = sns.scatterplot(data=diamonds, x=\"y\", y=\"z\", hue=\"price\", size=\"price\")\nax.set_xlim(xmin=3,xmax=12);\nax.set_ylim(ymin=1,ymax=8);\n\n\n\n\nFrom the three plots above, it appears that as the dimensions of the diamond increase, the price tends to increase as well. Let’s plot all three dimensions, along with price, to get a comprehensive view of how the dimensions affect price.\n\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,8))\nax = fig.add_subplot(projection='3d')\n\nscatter = ax.scatter(diamonds['x'], diamonds['y'], diamonds['z'], marker='o', c=diamonds['price'])\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_zlim3d(zmax=5)\nfig.colorbar(scatter, label=\"Price\", orientation='horizontal')\nplt.show()\n\n\n\n\nThe 3-D plot above supports our conclusion.\n\n\nConstructing a Nonlinear Model\nNow let’s try using a nonlinear model to predict the diamond price. We can focus on the DecisionTreeRegressor model. To avoid overfitting, let’s set the max_depth for this tree to be 2.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# First, split the data into a train set and a test set. \nX_train_nl, X_test_nl, y_train_nl, y_test_nl = train_test_split(X, y, test_size=0.10, random_state=42)\n\n# Construct the DecisionTreeRegressor model and fit the training data. \ntree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg.fit(X_train_nl, y_train_nl);\n\nJust like we did with our linear models, let’s compute the score for our model and predict the data in the test set.\n\ny_pred_nl = tree_reg.predict(X_test_nl)\ny_pred_nl\n\narray([ 1058.37929498,  6238.89262625, 12311.15592553, ...,\n       12311.15592553, 12311.15592553,  3251.84059003])\n\n\n\ntree_reg.score(X_test_nl, y_test_nl)\n\n0.8317539986753625\n\n\nOur decision tree has an R^2 value of around 83%. It looks like it performs similarly to the linear models we previously constructed.\nNow, let’s try visualizing our predictions against the true labels. We’ll just focus on the first 100 data points in order to make our plot less noisy.\n\n# ax = range(len(ytest))\nx_data = range(100)\nplt.plot(x_data, y_pred_nl[:100], label=\"predicted diamond price\")\nplt.plot(x_data, y_test_nl[:100], label=\"actual diamond price\")\nplt.title(\"actual diamond price and diamond price comparison\")\nplt.xlabel('price observation index')\nplt.ylabel('price value')\nplt.legend(loc='best',fancybox=True, shadow=True)\nplt.grid(True)\nplt.show()  \n\n\n\n\nIt seems that the predicted prices are more or less in the same range as the actual prices. However, it appears that the predictions tend to underestimate the price of expensive diamonds.\n\n\nAttributions\n\nhttps://www.educba.com/seaborn-datasets/\nhttps://www.kaggle.com/code/drvader/diamonds-dataset-exploration-and-regression\nhttps://github.com/Chinmayrane16/Diamonds-In-Depth-Analysis/blob/master/Diamonds.ipynb\nhttps://seaborn.pydata.org/generated/seaborn.scatterplot.html\nhttps://www.displayr.com/what-is-overplotting/\nhttps://www.geeksforgeeks.org/matplotlib-pyplot-colorbar-function-in-python/\nhttps://www.geeksforgeeks.org/3d-scatter-plotting-in-python-using-matplotlib/\nhttps://matplotlib.org/stable/gallery/mplot3d/scatter3d.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\nhttps://medium.com/the-code-monster/split-a-dataset-into-train-and-test-datasets-using-sk-learn-acc7fd1802e0\nhttps://corporatefinanceinstitute.com/resources/data-science/r-squared/\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\nhttps://www.datatechnotes.com/2020/10/regression-example-with-decisiontreeregressor.html"
  }
]