{
  "hash": "8d7d3321183e2bde50ed1fc38b852932",
  "result": {
    "markdown": "---\ntitle: This is a blog post on probability theory and random variables.\ndate: '2023-11-03'\ncategories:\n  - 'Probability Theory, Random Variables, Naive Bayes'\ndescription: 'This is a blog post on probability theory. Specifically, the Naive Bayes classifier is used.'\nexecute:\n  message: false\n  warning: false\neditor_options:\n  chunk_output_type: console\n---\n\nIn this blog post, we will investigate the Seaborn Attention dataset. Specifically, we will focus on using the Naive Bayes classifier to predict whether a test-taker is focused or not depending on their score. \n\n# Get and Examine the Data\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsns.get_dataset_names()\nattention_raw = sns.load_dataset('attention')\nattention_raw.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>subject</th>\n      <th>attention</th>\n      <th>solutions</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nattention_raw.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 60 entries, 0 to 59\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Unnamed: 0  60 non-null     int64  \n 1   subject     60 non-null     int64  \n 2   attention   60 non-null     object \n 3   solutions   60 non-null     int64  \n 4   score       60 non-null     float64\ndtypes: float64(1), int64(3), object(1)\nmemory usage: 2.5+ KB\n```\n:::\n:::\n\n\nLet's drop the Unnamed column, since it will not help with our analysis. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nattention = attention_raw.drop(\"Unnamed: 0\", axis=1)\nattention.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>attention</th>\n      <th>solutions</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>divided</td>\n      <td>1</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet's explore the dataset and see if there are any missing/outlier values. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nattention.isna().any().any()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nFalse\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nattention.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n(60, 4)\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n[attention['subject'].min(), attention['subject'].max()]\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n[1, 20]\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n[attention['solutions'].min(), attention['solutions'].max()]\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n[1, 3]\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n[attention['score'].min(), attention['score'].max()]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n[2.0, 9.0]\n```\n:::\n:::\n\n\nThere are 20 subjects total, who were given 3 different tests. For each of these tests, the score of the subjects was computed. The minimum score across all tests was 2.0, while the maximum score was 9.0.\n\n# Visualizations\n\nLet's now visualize the attention dataset. We will first display a kde plot, that shows the probability distribution of the scores for the focused and divided groups. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nsns.displot(data=attention, x='score', kde=True, hue='attention', stat='density');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=579 height=470}\n:::\n:::\n\n\nAs can be seen by this plot, the probability that a divided participant performed very well (i.e. got a score of 7 or higher) is substantially less than the probability that a focused user performed very well. Also, the probability that focused users performed poorly (i.e. got a score of 4 or lower) is substantially smaller than the probability that distracted users performed poorly. \n\nIt appears that focused users, on average, received better scores than non-focused users. Let's try to better visualize this trend by plotting subjects and their corresponding scores, and color-coding whether the subject was focused or not. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nsns.barplot(data=attention, x='subject', y='score', hue='attention');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=576 height=429}\n:::\n:::\n\n\nIt appears that, on average, focused subjects appeared to perform better than non-focused subjects. Let's confirm this hypothesis by computing the average scores for the divided and focused groups. \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\navg_scores = pd.DataFrame(attention.groupby('attention').mean()['score'])\navg_scores\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n    </tr>\n    <tr>\n      <th>attention</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>divided</th>\n      <td>5.116667</td>\n    </tr>\n    <tr>\n      <th>focused</th>\n      <td>6.800000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIndeed, we can see that the average score for divided users is around 5, while the average score for focused users is approximately 7. It appears that the average test score for focused users is about 2 points higher than that of distracted users.  \n\n\n# Naive Bayes Theory\n\nBefore getting hands-on experience using the Naive Bayes Classifier, it will be useful to explain the theory underlying this classifier. \n\nNaive Bayes is based on Bayes' Rule, which can be stated as follows: \n\n$$\\huge{P(A|B) = \\frac{P(A) P(B|A)}{P(B)}}$$\n\nWhere  \nP(A|B) is the probability that event A occurs given that event B has already occurred.  \nP(A) is the probability of event A occurring.  \nP(B|A) is the probability that event B occurs given that event A has already occurred.  \nP(B) is the probability of event B occurring. \n\nA Naive Bayes Classifier is designed to classify a given observation into one of several classes. In order to do so, the Naive Bayes Classifier relies heavily on Bayes theorem. Specifically, given a specific feature of the dataset, the Naive Bayes Classifier computes the probability of the observation falling into each of the different classes based on the value of the feature. Then, this classifier takes the highest such probability, and classifies the observation as falling into the class which matches with that largest probability. \n\nOne important consideration is that the Naive Bayes classifier assumes that features are conditionally independent of one another. This means that when using multiple features in order to predict which class an observation falls into, the Naive Bayes Classifier assumes that none of the features depend on each other. \n\n# Naive Bayes Classifier\n\nLet's now use a Naive Bayes classifier to predict whether a participant is distracted or not based on their score. We can drop the other columns, since they won't be relevant. \n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nattention_nb = attention[['attention', 'score']]\nattention_nb.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attention</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>divided</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>divided</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>divided</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>divided</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>divided</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNext, perform a train-test split. Since the dataset is small, let's choose 80% of the samples to be in the training set and 20% of the samples to be in the test set. \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\nX = attention_nb[['score']]\ny = attention_nb['attention']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nX_train.shape\nX_test.shape\ny_train.shape\ny_test.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n(12,)\n```\n:::\n:::\n\n\nNow, let's instantiate the model and fit the data to the model. We will use a Gaussian Naive Bayes classifier, which assumes that the distribution of continuous features is Gaussian. This appears to be a reasonable assumption in the case of the test scores. \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.naive_bayes import GaussianNB\n\nnb_gauss = GaussianNB()\nnb_gauss.fit(X_train, y_train);\n```\n:::\n\n\nLet's evaluate the accuracy on our testing set. \n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\npredictions = nb_gauss.predict(X_test)\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score as accuracy\nround(accuracy(y_test, predictions), 2)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n0.75\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\npredictions\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray(['divided', 'divided', 'focused', 'focused', 'focused', 'focused',\n       'focused', 'focused', 'focused', 'focused', 'focused', 'focused'],\n      dtype='<U7')\n```\n:::\n:::\n\n\n75% is actually quite good, considering how few samples were used to train the model. We can plot a confusion matrix to get a better sense for which samples the model misclassified. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nlabels_arr = ['divided', 'focused']\nconf_matrix = metrics.confusion_matrix(y_test, predictions, labels=labels_arr)\ncm = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['divided', 'focused'])\ncm.plot();\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){width=531 height=434}\n:::\n:::\n\n\nIt appears that the Gaussian NB classifier over-predicted samples as being focused. The three samples in the upper right quadrant were predicted as being focused, but were in fact divided. \n\n# Attributions\n\n1. https://seaborn.pydata.org/generated/seaborn.barplot.html\n2. https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\n3. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n4. https://scikit-learn.org/stable/modules/naive_bayes.html\n5. https://www.datacamp.com/tutorial/naive-bayes-scikit-learn\n6. https://corporatefinanceinstitute.com/resources/data-science/bayes-theorem/\n7. https://www.bayesrulesbook.com/chapter-14\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}