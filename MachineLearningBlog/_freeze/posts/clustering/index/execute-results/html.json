{
  "hash": "51937dbfefc4b0a0c9344ed43689be7e",
  "result": {
    "markdown": "---\ntitle: This is a blog post on clustering\ndate: '2022-10-29'\ncategories:\n  - Clustering\ndescription: This is a blog post on clustering.\nexecute:\n  message: false\n  warning: false\neditor_options:\n  chunk_output_type: console\n---\n\nIn this blog post, I will use the Seaborn \"Car Crashes\" dataset in order to group similar states together and observe patterns with motor accidents in these states. \n\n# Get and Examine the Data\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsns.get_dataset_names()\ncrashes = sns.load_dataset('car_crashes')\ncrashes.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total</th>\n      <th>speeding</th>\n      <th>alcohol</th>\n      <th>not_distracted</th>\n      <th>no_previous</th>\n      <th>ins_premium</th>\n      <th>ins_losses</th>\n      <th>abbrev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18.8</td>\n      <td>7.332</td>\n      <td>5.640</td>\n      <td>18.048</td>\n      <td>15.040</td>\n      <td>784.55</td>\n      <td>145.08</td>\n      <td>AL</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18.1</td>\n      <td>7.421</td>\n      <td>4.525</td>\n      <td>16.290</td>\n      <td>17.014</td>\n      <td>1053.48</td>\n      <td>133.93</td>\n      <td>AK</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18.6</td>\n      <td>6.510</td>\n      <td>5.208</td>\n      <td>15.624</td>\n      <td>17.856</td>\n      <td>899.47</td>\n      <td>110.35</td>\n      <td>AZ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22.4</td>\n      <td>4.032</td>\n      <td>5.824</td>\n      <td>21.056</td>\n      <td>21.280</td>\n      <td>827.34</td>\n      <td>142.39</td>\n      <td>AR</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12.0</td>\n      <td>4.200</td>\n      <td>3.360</td>\n      <td>10.920</td>\n      <td>10.680</td>\n      <td>878.41</td>\n      <td>165.63</td>\n      <td>CA</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe see that this dataset contains various details about accidents, like whether the driver was speeding or intoxicated. There is also the abbrev column, which indicates the state in which the accident took place. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ncrashes.shape\ncrashes[crashes['abbrev'] == \"DC\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total</th>\n      <th>speeding</th>\n      <th>alcohol</th>\n      <th>not_distracted</th>\n      <th>no_previous</th>\n      <th>ins_premium</th>\n      <th>ins_losses</th>\n      <th>abbrev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>5.9</td>\n      <td>2.006</td>\n      <td>1.593</td>\n      <td>5.9</td>\n      <td>5.9</td>\n      <td>1273.89</td>\n      <td>136.05</td>\n      <td>DC</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe reason there are 51 rows is because the 50 states along with DC are included. Now, let's check for missing or invalid data. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ncrashes.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 51 entries, 0 to 50\nData columns (total 8 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   total           51 non-null     float64\n 1   speeding        51 non-null     float64\n 2   alcohol         51 non-null     float64\n 3   not_distracted  51 non-null     float64\n 4   no_previous     51 non-null     float64\n 5   ins_premium     51 non-null     float64\n 6   ins_losses      51 non-null     float64\n 7   abbrev          51 non-null     object \ndtypes: float64(7), object(1)\nmemory usage: 3.3+ KB\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ncrashes.isna().any().any()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nFalse\n```\n:::\n:::\n\n\nLooks like there is no missing data. We can now perform a few checks on whether the data makes sense. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncrashes['total'].min()\ncrashes['speeding'].min()\ncrashes['alcohol'].min()\ncrashes['not_distracted'].min()\ncrashes['no_previous'].min()\ncrashes['ins_premium'].min()\ncrashes['ins_losses'].min()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n82.75\n```\n:::\n:::\n\n\nIt looks like the data is valid, so we are good to proceed. Let's first visualize the total column for each of the states. To make the barplot less cluttered, let's focus on the 5 states with the highest number of crashes and the 5 states with the lowest number of crashes. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ncrashes_sorted = crashes.sort_values('total', ascending=False)\ncrashes_sorted\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(data=crashes_sorted[:5], x='abbrev', y='total', ax=ax)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=808 height=580}\n:::\n:::\n\n\nThe states which have the highest number of crashes are South Carolina, North Dakota, West Virginia, Arkansas, and Kentucky. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(data=crashes_sorted[-5:], x='abbrev', y='total')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=808 height=577}\n:::\n:::\n\n\nOn the other end of the spectrum, Connecticut, Washington state, Minnesota, Massachussetts, and DC have the fewest crashes. \n\n# Dimensionality Reduction\n\nWe have 7 columns in our dataset, not including the state abbreviation. With so many columns, it is hard to visualize how similar data points are to one another. Therefore, we will need to perform dimensionality reduction before we can visualize similarities between different data points. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\npca = PCA(n_components=2)\n\n# Remove the abbreviation column, as it is not helpful in comparing data points. \ncrashes_without_abbrev = crashes.set_index('abbrev')\ncrashes_without_abbrev.loc[:, crashes_without_abbrev.columns != 'abbrev']\ncrashes_without_abbrev\n\ncrashes2D = pca.fit_transform(crashes_without_abbrev)\ncrashes2D = pd.DataFrame(crashes2D, index=crashes_without_abbrev.index)\npca.components_\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([[-4.58058550e-03, -8.80126113e-04, -1.65125559e-03,\n        -4.40130754e-03, -3.28217700e-03,  9.96137961e-01,\n         8.74901966e-02],\n       [ 2.70673578e-02, -1.44557187e-03,  3.67655984e-04,\n         1.27351225e-02,  2.53491114e-02, -8.71636336e-02,\n         9.95420953e-01]])\n```\n:::\n:::\n\n\nNow, let's see how much of the variance in the data is explained by the first 2 PCA dimensions. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\npca.explained_variance_ratio_\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([0.98671551, 0.01155058])\n```\n:::\n:::\n\n\nThe first component explains the vast majority of the variance (over 98%), while the second column explains almost all of the remaining variance. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n1 - pca.explained_variance_ratio_.sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.001733914130986669\n```\n:::\n:::\n\n\nIn fact, only around 0.17% of the variance is not explained by these first two components.  \nNow, let's visualize the data using a PCA plot. \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nax = crashes2D.plot.scatter(x=0, y=1, figsize=(10,7))\nfor ind,row in crashes2D.iterrows():\n    ax.text(row[0], row[1], ind)\nax.set_xlabel('PCA Component 1 (98.67%)');\nax.set_ylabel('PCA Component 2 (1.16%)');\n\n# crashes2D\n# for idx, row in crashes2D_df.iterrows():\n#    print(idx)\n#    print(row)\n    # ax.annotate(row['label_var'], (row['x_var'], row['y_var']))\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=819 height=577}\n:::\n:::\n\n\nThe plot above labels the states and plots them according to the two principal components. Even though accident totals is just one dimension, we can see that West Virginia, Arkansas, and Kentucky, three of the states that had high totals, are in the same approximate area.  \n\nNow, let's cluster the states together, using the K-Means algorithm. First, let's try to find the optimal number of clusters. \n\n## Elbow Curve\n\nLet's draw an elbow curve to visualize how the inertia drops as the number of clusters increases. \n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nkmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(crashes_without_abbrev)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.xlabel(\"$k$\");\nplt.ylabel(\"Inertia\");\nplt.grid()\nplt.show()\n# plt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n#              arrowprops=dict(facecolor='black', shrink=0.1))\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=663 height=333}\n:::\n:::\n\n\nFrom this plot, it appears that in fact k=2 is the inflection point of the graph. Let's also construct a silhouette plot. \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_score\nsilhouette_scores = [silhouette_score(crashes_without_abbrev, model.labels_)\n                     for model in kmeans_per_k[1:]]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(2, 10), silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Silhouette score\")\n\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=680 height=282}\n:::\n:::\n\n\nIt appears that 2 clusters is the best choice according to both the elbow curve and the silhouette score. There doesn't appear to be a good reason to not choose 2 clusters, so let's proceed with the KMeans algorithm using 2 clusters.  \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nkmeans = KMeans(n_clusters=2, random_state=42)\ny_pred = kmeans.fit_predict(crashes_without_abbrev)\ny_pred\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\narray([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0], dtype=int32)\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nax = crashes2D.plot.scatter(x=0, y=1, figsize=(10,7), c=y_pred, cmap='viridis', s=50)\nfor ind,row in crashes2D.iterrows():\n    ax.text(row[0], row[1], ind)\nax.set_xlabel('PCA Component 1 (98.67%)');\nax.set_ylabel('PCA Component 2 (1.16%)');\nax.axvline(x=85, color='r')\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n<matplotlib.lines.Line2D at 0x7fa0789b4640>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-2.png){width=764 height=582}\n:::\n:::\n\n\nFrom this plot, we can see a clear split along PC1. The vertical line on the plot separates the two clusters perfectly. We can see that states to the right of that line belong to one cluster and states to the left of the vertical line lie in the other cluster. Since PC1 explains over 98% of the variance, this approach would make sense.  \n\n# Attributions\n\n1. https://plainenglish.io/blog/9-seaborn-datasets-for-data-science-ml-beginners#3-car-crashes\n2. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html\n3. https://www.geeksforgeeks.org/how-to-set-a-seaborn-chart-figure-size/\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}