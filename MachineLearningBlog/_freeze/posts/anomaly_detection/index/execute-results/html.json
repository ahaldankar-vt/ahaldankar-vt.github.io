{
  "hash": "ec701b16b65dbf484d829e9984bd3833",
  "result": {
    "markdown": "---\ntitle: This is a blog post on anomaly detection.\ndate: '2023-10-31'\ncategories:\n  - Anomaly Detection\ndescription: 'This is a blog post on anomaly detection. Specifically, we will analyze the DBSCAN algorithm.'\nexecute:\n  message: false\n  warning: false\neditor_options:\n  chunk_output_type: console\n---\n\nIn this blog post, I will continue using the Seaborn \"Car Crashes\" dataset. Previously, we analyzed the states in this dataset by using the k-means clustering algorithm. In this blog post, we will continue with this analysis, but instead use the DBSCAN algorithm. DBSCAN will allow us to identify states which are outliers/anomalies. \n\n# Get and Examine the Data\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsns.get_dataset_names()\ncrashes = sns.load_dataset('car_crashes')\ncrashes.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total</th>\n      <th>speeding</th>\n      <th>alcohol</th>\n      <th>not_distracted</th>\n      <th>no_previous</th>\n      <th>ins_premium</th>\n      <th>ins_losses</th>\n      <th>abbrev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18.8</td>\n      <td>7.332</td>\n      <td>5.640</td>\n      <td>18.048</td>\n      <td>15.040</td>\n      <td>784.55</td>\n      <td>145.08</td>\n      <td>AL</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18.1</td>\n      <td>7.421</td>\n      <td>4.525</td>\n      <td>16.290</td>\n      <td>17.014</td>\n      <td>1053.48</td>\n      <td>133.93</td>\n      <td>AK</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18.6</td>\n      <td>6.510</td>\n      <td>5.208</td>\n      <td>15.624</td>\n      <td>17.856</td>\n      <td>899.47</td>\n      <td>110.35</td>\n      <td>AZ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22.4</td>\n      <td>4.032</td>\n      <td>5.824</td>\n      <td>21.056</td>\n      <td>21.280</td>\n      <td>827.34</td>\n      <td>142.39</td>\n      <td>AR</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12.0</td>\n      <td>4.200</td>\n      <td>3.360</td>\n      <td>10.920</td>\n      <td>10.680</td>\n      <td>878.41</td>\n      <td>165.63</td>\n      <td>CA</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIn the clustering blog post, we already established that this dataset has no NaN values and that it is ok to jump directly into analytics/visualizations. \n\nLet's perform the same PCA steps that we did in the clustering blog post. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\npca = PCA(n_components=2)\n\n# Remove the abbreviation column, as it is not helpful in comparing data points. \ncrashes_without_abbrev = crashes.set_index('abbrev')\ncrashes_without_abbrev.loc[:, crashes_without_abbrev.columns != 'abbrev']\ncrashes_without_abbrev\n\ncrashes2D = pca.fit_transform(crashes_without_abbrev)\ncrashes2D = pd.DataFrame(crashes2D, index=crashes_without_abbrev.index)\npca.explained_variance_ratio_\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\narray([0.98671551, 0.01155058])\n```\n:::\n:::\n\n\nFrom the explained variance ratio, we can see that nearly all of the variance (~98.7%) in the dataset is explained by the first principal component \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nax = crashes2D.plot.scatter(x=0, y=1, figsize=(10,7))\nfor ind,row in crashes2D.iterrows():\n    ax.text(row[0], row[1], ind)\nax.set_xlabel('PCA Component 1 (98.67%)');\nax.set_ylabel('PCA Component 2 (1.16%)');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=819 height=577}\n:::\n:::\n\n\nAbove is a plot which can help us better visualize where states lie along the two principal component axes. \n\nFirst, let's look for potential outliers by computing the Z-scores on our dataframe. Any states with z-score values that are larger than 2 or smaller than -2 are potential outliers.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nnorm_crashes = (crashes_without_abbrev - crashes_without_abbrev.mean()) / crashes_without_abbrev.std()\nnorm_crashes.head() # As we can see, the data has been normalized. \n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total</th>\n      <th>speeding</th>\n      <th>alcohol</th>\n      <th>not_distracted</th>\n      <th>no_previous</th>\n      <th>ins_premium</th>\n      <th>ins_losses</th>\n    </tr>\n    <tr>\n      <th>abbrev</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AL</th>\n      <td>0.730180</td>\n      <td>1.156638</td>\n      <td>0.435603</td>\n      <td>0.992425</td>\n      <td>0.274956</td>\n      <td>-0.574368</td>\n      <td>0.426272</td>\n    </tr>\n    <tr>\n      <th>AK</th>\n      <td>0.560360</td>\n      <td>1.200747</td>\n      <td>-0.209229</td>\n      <td>0.602537</td>\n      <td>0.799304</td>\n      <td>0.933964</td>\n      <td>-0.022674</td>\n    </tr>\n    <tr>\n      <th>AZ</th>\n      <td>0.681660</td>\n      <td>0.749253</td>\n      <td>0.185767</td>\n      <td>0.454831</td>\n      <td>1.022962</td>\n      <td>0.070177</td>\n      <td>-0.972106</td>\n    </tr>\n    <tr>\n      <th>AR</th>\n      <td>1.603542</td>\n      <td>-0.478849</td>\n      <td>0.542015</td>\n      <td>1.659539</td>\n      <td>1.932471</td>\n      <td>-0.334374</td>\n      <td>0.317961</td>\n    </tr>\n    <tr>\n      <th>CA</th>\n      <td>-0.919504</td>\n      <td>-0.395588</td>\n      <td>-0.882977</td>\n      <td>-0.588421</td>\n      <td>-0.883180</td>\n      <td>-0.047941</td>\n      <td>1.253703</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncols = norm_crashes.columns.values.tolist()\n\nnorm_outliers = set()\n\nfor ind,row in norm_crashes.iterrows():\n    for j in range(len(cols)):\n        if(abs(row[cols[j]]) > 2):\n            norm_outliers.add(ind)\nnorm_outliers\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n{'DC', 'HI', 'ID', 'LA', 'MD', 'MS', 'MT', 'ND', 'NJ', 'PA', 'SC', 'WV'}\n```\n:::\n:::\n\n\nWe get the following list of states that are potential outliers. As we can see from the PCA plot, some of these states/districts, like DC, LA, and MT, lie on the outskirts of the plot and aren't close to other states.  \n\nHowever, picking an arbitrary z-score threshold to detect outliers is a somewhat crude method. Let's now use the DBSCAN algorithm to identify outliers. \n\n## DBSCAN Algorithm\n\nNow, we are ready to use the DBScan algorithm to identify the outlier states. This algorithm takes in two parameters: eps and min_samples. This algorithm finds core points and then expands outwards from those core points to form clusters.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=30, min_samples=2)\ndbscan.fit(crashes_without_abbrev)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DBSCAN(eps=30, min_samples=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DBSCAN</label><div class=\"sk-toggleable__content\"><pre>DBSCAN(eps=30, min_samples=2)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndbscan.labels_\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([ 0,  1,  2,  0,  3,  0, -1,  4, -1,  4,  3,  2,  5,  0,  5,  5,  0,\n        2, -1,  5, -1,  1,  4,  0,  3,  0,  0,  5,  1,  5, -1,  2, -1,  5,\n        5,  5,  3,  0,  3,  4,  2,  5,  0,  1,  0,  5,  0,  2,  1,  5,  0])\n```\n:::\n:::\n\n\nWe had to make our eps quite large and the min. samples small in order to get meaningful clusters. Otherwise, there were just too many outliers. \n\nLet's now plot these cluster assignments on our original PCA plot. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nax = crashes2D.plot.scatter(x=0, y=1, figsize=(10,7), c=dbscan.labels_, cmap='tab10', s=50)\nfor ind,row in crashes2D.iterrows():\n    ax.text(row[0], row[1], ind)\nax.set_xlabel('PCA Component 1 (98.67%)');\nax.set_ylabel('PCA Component 2 (1.16%)');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=762 height=582}\n:::\n:::\n\n\nLet's try to get a list of the outliers identified by the DBSCAN algorithm. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\noutliers = [crashes2D.index[i] for i in range(len(dbscan.labels_)) if dbscan.labels_[i] == -1]\noutliers\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n['CT', 'DC', 'LA', 'MD', 'NJ', 'NY']\n```\n:::\n:::\n\n\nThe DBSCAN algorithm, with eps of 30 and min_samples of 2, identifies 6 states/districts above as outliers ('CT', 'DC', 'LA', etc.). \n\nIt seems that from the plot, some states, like LA and MD, are so far from the rest of the states that they are clear outliers. Additionally, even though 'CT' might appear close to the red cluster, remember that the PCA plot is a 2-D representation of high-dimensional data. Therefore, 'CT' is actually probably much further away from the red cluster in reality. \n\n## Gaussian Mixtures Algorithm\n\nThe Gaussian Mixture algorithm assumes that the data is a combination of several different normal (Gaussian) distributions. A given data point will be added to a cluster if it has the highest probability of belonging to that cluster as opposed to other clusters.  \n\nLet's try running the Gaussian Mixture Algorithm in order to determine the anomalies in the crashes dataset. We can tune the value of num_components (i.e. number of clusters). Also, let's set n_inits to be 10. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.mixture import GaussianMixture\ngm = GaussianMixture(n_components=2, n_init=10, random_state=42)\ngm.fit(crashes_without_abbrev)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(n_components=2, n_init=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture(n_components=2, n_init=10, random_state=42)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nLet's take a look at the parameters that were obtained from the training process. \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ngm.weights_\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\narray([0.57719335, 0.42280665])\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ngm.means_\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\narray([[  16.37076754,    4.90459755,    5.09393917,   13.59724703,\n          14.457428  ,  774.68844487,  120.18638301],\n       [  14.99763053,    5.12597186,    4.60398743,   13.54031661,\n          13.3870909 , 1040.22164212,  154.02396378]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ngm.converged_\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nTrue\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ngm.n_iter_\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n21\n```\n:::\n:::\n\n\nIt looks like the algorithm converged after 21 iterations. Now, let's take a look at the predicted class labels as well as the probability distribution between the two classes. \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ngm_labels = gm.predict(crashes_without_abbrev)\ngm_labels\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n       1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0])\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ngm.predict_proba(crashes_without_abbrev).round(3)[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray([[0.091, 0.909],\n       [0.009, 0.991],\n       [1.   , 0.   ],\n       [0.947, 0.053],\n       [0.013, 0.987]])\n```\n:::\n:::\n\n\nAs we can see, there are two classes, with labels 0 and 1. The probability that each of the first 5 samples belongs to class 0 and class 1, respectively, are shown in the cell above.  \n\nLet's now try to visualize the cluster assignments given by the Gaussian Mixture on the PCA plot. \n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nax = crashes2D.plot.scatter(x=0, y=1, figsize=(10,7), c=gm_labels, cmap='viridis', s=50)\nfor ind,row in crashes2D.iterrows():\n    ax.text(row[0], row[1], ind)\nax.set_xlabel('PCA Component 1 (98.67%)');\nax.set_ylabel('PCA Component 2 (1.16%)');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){width=764 height=582}\n:::\n:::\n\n\nUnsurprisingly, the samples are again split primarily based off of Principal Component 1. \n\n## Gaussian Mixtures for Anomaly Detection\n\nWe can use the trained Gaussian Mixture model in order to predict which observations are outliers. Visually, it appears that there should be at least 2 or 3 outliers. As a first attempt, we can set a density threshold at the 5th percentile. This would account for approximately 2-3 samples (0.05 * 51 = 2.55). All of the samples that fall below this threshold will be classified as outliers. \n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nimport numpy as np\n\ndensities = gm.score_samples(crashes_without_abbrev)\ndensity_threshold = np.percentile(densities, 5)\nanomalies_below = crashes_without_abbrev[densities < density_threshold]\nanomalies_below\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total</th>\n      <th>speeding</th>\n      <th>alcohol</th>\n      <th>not_distracted</th>\n      <th>no_previous</th>\n      <th>ins_premium</th>\n      <th>ins_losses</th>\n    </tr>\n    <tr>\n      <th>abbrev</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>KY</th>\n      <td>21.4</td>\n      <td>4.066</td>\n      <td>4.922</td>\n      <td>16.692</td>\n      <td>16.264</td>\n      <td>872.51</td>\n      <td>137.13</td>\n    </tr>\n    <tr>\n      <th>MS</th>\n      <td>17.6</td>\n      <td>2.640</td>\n      <td>5.456</td>\n      <td>1.760</td>\n      <td>17.600</td>\n      <td>896.07</td>\n      <td>155.77</td>\n    </tr>\n    <tr>\n      <th>ND</th>\n      <td>23.9</td>\n      <td>5.497</td>\n      <td>10.038</td>\n      <td>23.661</td>\n      <td>20.554</td>\n      <td>688.75</td>\n      <td>109.72</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWhen using Gaussian Mixtures, Kentucky, Mississippi, and North Dakota are classified as outliers. This is an interesting result, since DBSCAN identified a different set of outliers.  \n\nOne possible explanation, is that DBSCAN has difficulty with sparse points, while Gaussian Mixtures is more robust to sparse points. Gaussian Mixtures instead focuses on the probability that a sample belongs to a given cluster or not. Hence, the DBSCAN and Gaussian Mixtures algorithms may identify a different set of outliers in some cases. \n\n# Attributions\n\n1. https://www.w3schools.com/python/pandas/ref_df_iterrows.asp\n2. https://en.wikipedia.org/wiki/DBSCAN\n3. https://sparkbyexamples.com/pandas/pandas-get-column-names/\n4. https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n5. https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture\n6. https://www.geeksforgeeks.org/gaussian-mixture-model/\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}