{
  "hash": "1be6cb1ca3143fdf57296f2c24b07b8c",
  "result": {
    "markdown": "---\ntitle: This is a blog post on classification\ndate: '2022-10-21'\ncategories:\n  - Classification\ndescription: 'This is a blog post on classification. Specifically, the SVM classifier is used.'\nexecute:\n  message: false\n  warning: false\neditor_options:\n  chunk_output_type: console\n---\n\n# Get and examine data\n\n## Download data\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Necessary data science packages\nimport sys\nfrom packaging import version\nimport sklearn\nimport sklearn.datasets\nimport pandas as pd\nimport numpy as np\n\nwine_dataset = sklearn.datasets.load_wine()\nwine_df = pd.DataFrame(data=wine_dataset.data, columns=wine_dataset.feature_names)\nwine_df['labels'] = wine_dataset['target'] # Also add the labels associated with each sample\n\n# Added code - maybe take out. \nX, y = wine_dataset.data, wine_dataset.target\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\narray([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n        1.065e+03],\n       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n        1.050e+03],\n       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n        1.185e+03],\n       ...,\n       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n        8.350e+02],\n       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n        8.400e+02],\n       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n        5.600e+02]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nX.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(178, 13)\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ny.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(178,)\n```\n:::\n:::\n\n\n## Inspect the data\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Display the head of the wine dataframe. \nwine_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Display column names and types\nwine_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       178 non-null    float64\n 1   malic_acid                    178 non-null    float64\n 2   ash                           178 non-null    float64\n 3   alcalinity_of_ash             178 non-null    float64\n 4   magnesium                     178 non-null    float64\n 5   total_phenols                 178 non-null    float64\n 6   flavanoids                    178 non-null    float64\n 7   nonflavanoid_phenols          178 non-null    float64\n 8   proanthocyanins               178 non-null    float64\n 9   color_intensity               178 non-null    float64\n 10  hue                           178 non-null    float64\n 11  od280/od315_of_diluted_wines  178 non-null    float64\n 12  proline                       178 non-null    float64\n 13  labels                        178 non-null    int64  \ndtypes: float64(13), int64(1)\nmemory usage: 19.6 KB\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Get the value counts for each different type of wine. \nwine_df['labels'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nlabels\n1    71\n0    59\n2    48\nName: count, dtype: int64\n```\n:::\n:::\n\n\nIt seems that the labels are roughly balanced, although wine type #1 is the most common. Now, let's do a train-test split. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef shuffle_and_split_data(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntrain_set, test_set = shuffle_and_split_data(wine_df, 0.2)\nlen(train_set)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n143\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Allows results of the notebook to be reproducible. \nnp.random.seed(42)\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nlen(test_set)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n35\n```\n:::\n:::\n\n\n## Train Test Split\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nX_train.shape\nX_test.shape\ny_train.shape\ny_test.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n(59,)\n```\n:::\n:::\n\n\n## Training and Predictions\n\nWe have successfully performed a train-test split with 143 samples in the training set and 35 samples in the test set.\nSince we are dealing with a small dataset for multi-class classification, it might be helpful to use the SVM classifier. \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.svm import SVC\n\n# Instantiate and fit on the training set. \nsvm_clf = SVC(random_state=42)\nsvm_clf.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Predict on the testing set. \npredictions = svm_clf.predict(X_test)\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Now, let's evaluate the accuracy of the predictions. \nfrom sklearn.metrics import accuracy_score as accuracy\nround(accuracy(y_test, predictions), 2)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n0.71\n```\n:::\n:::\n\n\nWe get an accuracy of around 71%. Let's plot our results in a confusion matrix. \n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nconf_matrix = metrics.confusion_matrix(y_test, predictions)\ncm = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[0,1,2])\ncm.plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7faf62d6daf0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-2.png){width=509 height=434}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}