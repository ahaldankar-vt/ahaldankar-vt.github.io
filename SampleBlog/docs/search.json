[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SampleBlog",
    "section": "",
    "text": "This is a blog post on classification\n\n\n\n\n\n\n\nClassification\n\n\n\n\nThis is a blog post on classification. Specifically, the SVM classifier is used.\n\n\n\n\n\n\nOct 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\nThis is a blog post on regression.\n\n\n\n\n\n\n\n123\n\n\nSecond Tag\n\n\n\n\nThis is a blog post on regression. I will use the Seaborn Diamonds dataset to predict the price of a diamond based on other features about it.\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\nNo matching items\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "This is a blog post on classification",
    "section": "",
    "text": "# Necessary data science packages\nimport sys\nfrom packaging import version\nimport sklearn\nimport sklearn.datasets\nimport pandas as pd\nimport numpy as np\n\nwine_dataset = sklearn.datasets.load_wine()\nwine_df = pd.DataFrame(data=wine_dataset.data, columns=wine_dataset.feature_names)\nwine_df['labels'] = wine_dataset['target'] # Also add the labels associated with each sample\n\n# Added code - maybe take out. \nX, y = wine_dataset.data, wine_dataset.target\nX\n\narray([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n        1.065e+03],\n       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n        1.050e+03],\n       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n        1.185e+03],\n       ...,\n       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n        8.350e+02],\n       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n        8.400e+02],\n       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n        5.600e+02]])\n\n\n\nX.shape\n\n(178, 13)\n\n\n\ny\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])\n\n\n\ny.shape\n\n(178,)\n\n\n\n\n\n\n# Display the head of the wine dataframe. \nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nlabels\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0\n\n\n\n\n\n\n\n\n# Display column names and types\nwine_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       178 non-null    float64\n 1   malic_acid                    178 non-null    float64\n 2   ash                           178 non-null    float64\n 3   alcalinity_of_ash             178 non-null    float64\n 4   magnesium                     178 non-null    float64\n 5   total_phenols                 178 non-null    float64\n 6   flavanoids                    178 non-null    float64\n 7   nonflavanoid_phenols          178 non-null    float64\n 8   proanthocyanins               178 non-null    float64\n 9   color_intensity               178 non-null    float64\n 10  hue                           178 non-null    float64\n 11  od280/od315_of_diluted_wines  178 non-null    float64\n 12  proline                       178 non-null    float64\n 13  labels                        178 non-null    int64  \ndtypes: float64(13), int64(1)\nmemory usage: 19.6 KB\n\n\n\n# Get the value counts for each different type of wine. \nwine_df['labels'].value_counts()\n\nlabels\n1    71\n0    59\n2    48\nName: count, dtype: int64\n\n\nIt seems that the labels are roughly balanced, although wine type #1 is the most common. Now, let’s do a train-test split.\n\ndef shuffle_and_split_data(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n\ntrain_set, test_set = shuffle_and_split_data(wine_df, 0.2)\nlen(train_set)\n\n143\n\n\n\n# Allows results of the notebook to be reproducible. \nnp.random.seed(42)\n\n\nlen(test_set)\n\n35\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\nX_train.shape\nX_test.shape\ny_train.shape\ny_test.shape\n\n(59,)\n\n\n\n\n\nWe have successfully performed a train-test split with 143 samples in the training set and 35 samples in the test set. Since we are dealing with a small dataset for multi-class classification, it might be helpful to use the SVM classifier.\n\nfrom sklearn.svm import SVC\n\n# Instantiate and fit on the training set. \nsvm_clf = SVC(random_state=42)\nsvm_clf.fit(X_train, y_train)\n\nSVC(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(random_state=42)\n\n\n\n# Predict on the testing set. \npredictions = svm_clf.predict(X_test)\n\n\n# Now, let's evaluate the accuracy of the predictions. \nfrom sklearn.metrics import accuracy_score as accuracy\nround(accuracy(y_test, predictions), 2)\n\n0.71\n\n\nWe get an accuracy of around 71%. Let’s plot our results in a confusion matrix.\n\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nconf_matrix = metrics.confusion_matrix(y_test, predictions)\ncm = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[0,1,2])\ncm.plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7faf62d6daf0&gt;"
  },
  {
    "objectID": "posts/classification/index.html#download-data",
    "href": "posts/classification/index.html#download-data",
    "title": "This is a blog post on classification",
    "section": "",
    "text": "# Necessary data science packages\nimport sys\nfrom packaging import version\nimport sklearn\nimport sklearn.datasets\nimport pandas as pd\nimport numpy as np\n\nwine_dataset = sklearn.datasets.load_wine()\nwine_df = pd.DataFrame(data=wine_dataset.data, columns=wine_dataset.feature_names)\nwine_df['labels'] = wine_dataset['target'] # Also add the labels associated with each sample\n\n# Added code - maybe take out. \nX, y = wine_dataset.data, wine_dataset.target\nX\n\narray([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n        1.065e+03],\n       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n        1.050e+03],\n       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n        1.185e+03],\n       ...,\n       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n        8.350e+02],\n       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n        8.400e+02],\n       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n        5.600e+02]])\n\n\n\nX.shape\n\n(178, 13)\n\n\n\ny\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])\n\n\n\ny.shape\n\n(178,)"
  },
  {
    "objectID": "posts/classification/index.html#inspect-the-data",
    "href": "posts/classification/index.html#inspect-the-data",
    "title": "This is a blog post on classification",
    "section": "",
    "text": "# Display the head of the wine dataframe. \nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nlabels\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0\n\n\n\n\n\n\n\n\n# Display column names and types\nwine_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       178 non-null    float64\n 1   malic_acid                    178 non-null    float64\n 2   ash                           178 non-null    float64\n 3   alcalinity_of_ash             178 non-null    float64\n 4   magnesium                     178 non-null    float64\n 5   total_phenols                 178 non-null    float64\n 6   flavanoids                    178 non-null    float64\n 7   nonflavanoid_phenols          178 non-null    float64\n 8   proanthocyanins               178 non-null    float64\n 9   color_intensity               178 non-null    float64\n 10  hue                           178 non-null    float64\n 11  od280/od315_of_diluted_wines  178 non-null    float64\n 12  proline                       178 non-null    float64\n 13  labels                        178 non-null    int64  \ndtypes: float64(13), int64(1)\nmemory usage: 19.6 KB\n\n\n\n# Get the value counts for each different type of wine. \nwine_df['labels'].value_counts()\n\nlabels\n1    71\n0    59\n2    48\nName: count, dtype: int64\n\n\nIt seems that the labels are roughly balanced, although wine type #1 is the most common. Now, let’s do a train-test split.\n\ndef shuffle_and_split_data(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n\ntrain_set, test_set = shuffle_and_split_data(wine_df, 0.2)\nlen(train_set)\n\n143\n\n\n\n# Allows results of the notebook to be reproducible. \nnp.random.seed(42)\n\n\nlen(test_set)\n\n35"
  },
  {
    "objectID": "posts/classification/index.html#train-test-split",
    "href": "posts/classification/index.html#train-test-split",
    "title": "This is a blog post on classification",
    "section": "",
    "text": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\nX_train.shape\nX_test.shape\ny_train.shape\ny_test.shape\n\n(59,)"
  },
  {
    "objectID": "posts/classification/index.html#training-and-predictions",
    "href": "posts/classification/index.html#training-and-predictions",
    "title": "This is a blog post on classification",
    "section": "",
    "text": "We have successfully performed a train-test split with 143 samples in the training set and 35 samples in the test set. Since we are dealing with a small dataset for multi-class classification, it might be helpful to use the SVM classifier.\n\nfrom sklearn.svm import SVC\n\n# Instantiate and fit on the training set. \nsvm_clf = SVC(random_state=42)\nsvm_clf.fit(X_train, y_train)\n\nSVC(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(random_state=42)\n\n\n\n# Predict on the testing set. \npredictions = svm_clf.predict(X_test)\n\n\n# Now, let's evaluate the accuracy of the predictions. \nfrom sklearn.metrics import accuracy_score as accuracy\nround(accuracy(y_test, predictions), 2)\n\n0.71\n\n\nWe get an accuracy of around 71%. Let’s plot our results in a confusion matrix.\n\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nconf_matrix = metrics.confusion_matrix(y_test, predictions)\ncm = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[0,1,2])\ncm.plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7faf62d6daf0&gt;"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]